\documentclass[11pt,a4paper]{report}
\usepackage[utf8]{inputenc}
\usepackage[french]{babel}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{parskip}
\usepackage{fullpage}
\usepackage{amsthm}
\usepackage{etoolbox}
\usepackage{tcolorbox}
\usepackage{multirow}
\usepackage[ruled]{algorithm2e}

\BeforeBeginEnvironment{minted}%
     {\begin{tcolorbox}}%
\AfterEndEnvironment{minted}
   {\end{tcolorbox}}%

\tcbuselibrary{theorems}

\newtcbtheorem[]{mytheo}{Théorème}%
{colback=green!5,colframe=green!35!black,fonttitle=\bfseries}{th}

\usepackage{minted}
\usemintedstyle{manni}
\newminted{c}{
    tabsize=4,
    fontsize=\small,
    gobble=4
}

\author{Sylvain Julmy}
\title{PCM - Résumé}

\begin{document}
\chapter{Introduction} % (fold)
\label{cha:Introduction}

Pourquoi faire du multicoeurs :
\begin{itemize}
    \item Limite thermique
    \item Limite des accès mémoires
    \item Limite de l'intégration (mettre $\approx 10^{12}$ mots sur une surface de $0.3^2mm^2$)
\end{itemize}

La solution est le CMP, une architecture MIMD (Multiple Instruction Stream, Multiple Data stream),
il s'agit d'un ensemble de thread qui exécute un graphe de précédence.

\section{Loi d'Amdhal} % (fold)
\label{sec:Loi d'Amdhal}
Le gain de vitesse (speed-up) pour une architecture à $n$ coeurs est donné par
$S(n)=\frac{T_1}{T_n}$ où $T_n$ est le temps pour exécuter le problème avec $n$ coeurs. Idéalement,
il faudrait avoir $S(n)=n$ : aller $n$ fois plus vite. Mais c'est rarement le cas !

Tout programme contient une partie séquentielle $seq$ et une partie $par$ pouvant être exécutée en
parallèle. Soit $p$ la partie séquentielle du problème : $p=\frac{seq}{seq+par}$.
$$
S(n) = \frac{T_1}{T_1p+\frac{(1-p)T_1}{n}} = \frac{1}{p+\frac{1-p}{n}}
$$

\paragraph*{Exemple : } $60\%$ concurrent et $40\%$ séquentielle : $S(10) =
\frac{1}{0.4+\frac{0.6}{10}} = 2.17$, $S(10) = \frac{1}{0.99+\frac{0.01}{10}} = 9.17$.

Conclusion : le petit $\%$ séquentielle influence fortement le gain en vitesse fourni par un CMP.
% section Loi d'Amdhal (end)

\section{Efficacité} % (fold)
\label{sec:Efficacité}

L'efficacité $E(n)$ est l'utilisation moyenne des $n$ coeurs pour exécuter l'application :
$E(n)=\frac{S(n)}{n}$. $E(n) = 1$ implique que $S(n) = n$. Généralement une augmentation de $S(n)$
(en donnant des coeurs à l'application) se réalise au détriment de l'efficacité.
% section Efficacité (end)

\section{Caractérisation du parallélisme} % (fold)
\label{sec:Caractérisation du parallélisme}

\begin{itemize}
    \item $p$ : fraction séquentiel de l'application
    \item $m_{min}$
    \item $m_{max}$
    \item $p_i$ proportion de l'exécution avec $i$ coeurs
    \item $A$ : paralélisme moyen de l'application 
\end{itemize}

$$A = \sum^{m_{max}}_{i=m_{min}}i p_i$$

$A$ peut se définir comme étant
\begin{itemize}
    \item égal au nombre moyen de coeurs utilisés durant l'exécution de l'application si $n \geq
        m_{max}$.
    \item égal au gain de vitesse $S(\infty)=\frac{T_1}{T_{\infty}}=\frac{A
        T{_\infty}}{T_{\infty}}$.
    \item le rapport entre la somme du temps d'exécution de toutes les actions du graphe de
        précédence de l'application et le chemin le plus long de la racine aux feuilles de ce
        graphe.
\end{itemize}
% section Caractérisation du parallélisme (end)

\section{Relation entre $S(n)$ et $A$} % (fold)
\label{sec:Relation entre $S(n)$ et $A$}

Soit $A$ le parallélisme moyen d'une application, $S(n)$ son gain de vitesse pour $n$ coeurs et $E(n)$ l'efficacité des $n$ coeurs. Alors $S(n) \geq \frac{nA}{n+A-1}$ et $E(n) \geq \frac{A}{n+A-1}$.

% section Relation entre $S(n)$ et $A$ (end)

La programmation concurrente multicoeurs est difficile :
\begin{itemize}
    \item Limitation de la loi d'Amdhal.
    \item Idéalement il faut maintenir une efficacité élevée.
    \item Le dimensionnement de la granularité de calcul est difficile (granularité = exécution fait
        entre 2 points de synchronisation).
    \item la localité des données freine la vitesse (conserver le maximum de données en
        antémémoire).
    \item le balancement de la charge de chaque thread n'est pas évident.
    \item le partage de données et la synchronisation entre les threads doivent être rapide.
\end{itemize}
% chapter Introduction (end)

\chapter{Exclusion mutuelle} % (fold)
\label{cha:Exclusion mutuelle}
Un thread est une suite ordonnée d'événemments. Si $a_0$ et $a_1$ sont deux événements d'un thread, on peut les représenter schématiquement sur l'axe du temps. Un intervalle $A=(a,b)$ est l'activité (ensemble d'événements) compris entre deux événements spécifiques $a$ et $b$.

Un événement $a$ précède $b$ si $a$ se produit avant $b$ : se note $a \rightarrow b$. Un intervalle $A=(a_0,a_1)$ précède $B=(b_0,b_1)$, si $a_1 \rightarrow b_0$ ou $a_1 = b_0$. Propriétés :
\begin{itemize}
    \item irréflexive $A \rightarrow A$ est toujours faux
    \item antisymmétrique : si $A \rightarrow B$ alors $B \rightarrow A$ est faux
    \item transitive : si $A \rightarrow B \vee B \rightarrow C$ alors $A \rightarrow C$
\end{itemize}

La précédence ne peut être définis si $A$ et $B$ sont dans des threads différents. Les intervalles qui représentes les sections critiques de deux programmes ne peuvent pas se chevaucher.

\section{Algorithme de Peterson} % (fold)
\label{sec:Algorithme de Peterson}

\begin{ccode}
    volatile boolean_t flag[] = {FALSE,FALSE};
    volatile unsigned victim;

    cilk int Thread(unsigned i) // i = 0 ou 1
    {
        unsigned j = 1 - i;
        flag[i] = TRUE;
        victim = i;
        Cilk_fence();

        while (flag[j] && victim == i);
        // section critique
        flag[i] = FALSE;

        Cilk_fence();
        return 0;
    }
\end{ccode}
% section Algorithme de Peterson (end)

L'algorithme de Peterson garanti l'exclusion mutuelle. Cet algorithme est dépourvue d'interblocage (un interblocage surgit quand aucun thread ne peut progresser), pour qu'il en ai un dans notre cas, il faudrait que les deux threads soient pris dans leur bloucle respective. L'algorithme de Peterson est aussi dépouvue de famine.

Pour $n$ threads, on crée $n+1$ niveaux, quand un thread est en dehors de sa section critique, il est au niveau $0$. Si un thread souhaite entrer en section critique, il doit franchir les $n$ niveaux restant. \`{A} chaque niveaux $i$ il ne peut y avoir que $n-i+1$ threads au maximum.

\begin{ccode}
    unsigned n;         // nombre de threads
    unsigned level[n];  // niveau courant du thread i (ini à 0)
    unsigned victim[n]; // victime au niveau L

    cilk int Thread(unsigned i) // i = 0..n-1
    { 
        unsigned k, L;
        for (L = 1; L < n; L += 1)
        {
            level[i] = L;
            victim[L] = i;
            Cilk_fence();

            for (k = 0; k < n; )
            {
                if (k != i && level[k] >= L && victim[L] == i)
                    k = 0;
                else
                    k += 1;
            }
        }
        // section critique
        level[i] = 0;

        Cilk_fence();
        return 0;
    } 
\end{ccode}

Affirmations :
\begin{itemize}
    \item Quand $T_i$ passe au du niveau $L$ au niveau $L+1$, uniquement une des deux conditions est possible :
        \begin{itemize}
            \item C1 : $T_i$ précède tous les autres threads
            \item C2 : $T_i$ n'est pas le seul au niveau $L$
        \end{itemize}
    \item S'il y a plus d'un thread au niveau $L$, il doit y avoir au moins $1$ threads dans tous les niveaux $1$ à $L-1$
    \item Il y a au plus $n-L+1$ threads au niveau $L$
\end{itemize}

\section{Algorithme de la boulangerie} % (fold)
\label{sec:Algorithme de la boulangerie}

Idée : chaque thread choisit un numéro en entrant qui reflète son ordre de passage (comme à la poste).

L'algorithme ne s'appuie sur aucun dispositif centralisé et chaque thread choisit son propre numéro en fonction de ceux pris par les autres. En cas d'égalité, il faut départager les threads :
$$
n_i << n_j \text{ si } n_i < n_j \vee (n_i = n_j \wedge i < j)
$$

avec $n_i$ et $n_j$ sont des numéros tirés respectivement par les threads $i$ et $j$.

L'algorothme dispose de deux tableaux : \texttt{flag[i]} (état du thread $i$) et \texttt{label[i]} (numéro tiré par thread $i$). Avec $0$ initilement dans toutes les cases de tous les threads.

\begin{ccode}
    unsigned threads;        // nombre de threads
    boolean_t flag[threads]; // état courant du thread i
    int64_t label[threads];  // numéro pris par le thread i

    cilk int Thread(unsigned i) // i = 0..threads-1
    {
        unsigned j, k;
        int64_t max;
        flag[i] = TRUE;

        for (max = label[0], k = 1; k < threads; k += 1)
            if (label[k] > max) max = label[k];
                label[i] = max + 1;

        Cilk_fence();

        for (k = 0; k < threads; )
            if (flag[k] && label[i] >> label[k])
                k = 0;
            else
                k += 1;

        // SC
        flag[i] = FALSE;

        Cilk_fence();
        return 0;
    } 
\end{ccode}

L'algorithme préserve l'exclusion mutuelle (voir slides $20$ - $21$). Il n'y a pas d'interblocage : \texttt{label[A] >> label[B]} pour $A$ et \texttt{label[B] >> label[A]} pour $B$, donc impossible. L'algorithme garantit un traitement FIFO. C'est un algorithme simple et équitable mais pas pratique : il faut lire $n$ variables différentes. Si $n$ est grand, la pénalité est grande (loi d'Amdahl).

\begin{center}
    \begin{tabular}{cc|c|c|}
    \cline{3-4}
                                                      &            & \multicolumn{2}{c|}{writer (W)} \\ \cline{3-4} 
                                                      &            & single (S)      & multi (M)     \\ \hline
    \multicolumn{1}{|c|}{\multirow{2}{*}{reader (R)}} & single (S) & SRSW            & SRMW          \\ \cline{2-4} 
    \multicolumn{1}{|c|}{}                            & multi (M)  & MRSW            & MRMW          \\ \hline
    \end{tabular}
\end{center}

\begin{mytheo}{Variables partagées}{var-par}
    Au moins $n$ MRSW variables partagées sont nécessaire pour résoudre le problème de l'exclusion mutuelle avec $n$ threads
\end{mytheo}

% section Algorithme de la boulangerie (end)

\section{Verrou TAS} % (fold)
\label{sec:Verrou TAS}

Certaines architectures offrent une instruction atomique mettant un mot à vrai et retournant sa valeur précédente :

\begin{ccode}
    bool TestAndSet(bool* adr)
    {
        bool val = *adr;
        *adr = true;
        return val;
    }
\end{ccode}

Avec cela on peut faire un verrou (verrou TAS) :

\begin{ccode}
    bool lock = false; // lock

    void TAS_AcquireLock(bool* lock)
    {
        while(TestAndSet(lock));
    }

    void TAS_RealeaseLock(bool* lock)
    {
        *lock = false;
    }
\end{ccode}

Propriétés :
\begin{itemize}
    \item Nécessite $1$ booléen par verrou (indépendant du nombre de threads)
    \item Famine possible en théorie mais improbable en pratique
    \item Pas FIFO
    \item Si l'architecture a des antémémoires cohérentes, chaque appel à \texttt{TestAndSet} crée une invalidation des antémémoires ce qui augemente le trafic sur les bus.
\end{itemize}

% section Verrou TAS (end)

\section{Verrou TATAS} % (fold)
\label{sec:Verrou TATAS}

\begin{ccode}
    bool lock = false; // verrou
    void TATAS_AcquireLock(bool *lock)
    {
        while (true)
        {
            while (*lock);
            if (!TestAndSet(lock))
                return;
        }
    } 

    void TATAS_ReleaseLock(bool *lock)
    {
        *lock = false;
    } 
\end{ccode}

TATAS est logiquement équivalent à TAS mais diffère en performance. Environ $25\%$ plus rapide sur un Core 2 Duo avec $2$ threads.

% section Verrou TATAS (end)

\section{Verrou TATAS avec attente} % (fold)
\label{sec:Verrou TATAS avec attente}

\begin{ccode}
    void TATASBackoffAcquireLock(bool *lock)
    {
        int delay;

        while (*lock);
            if (!TestAndSet(lock))
                return;

        delay = random(MIN_DELAY,MAX_DELAY);

        while (true)
        {
            sleep(delay);
            while (*lock);
                if (!TestAndSet(lock))
                    return;
            delay = min(2*delay,MAX_WAIT);
        }
    } 
\end{ccode}

Si le \texttt{TestAndSet()} de TATAS ne réussit pas, c'est qu'il y a d'autre threads qui sollicitent le verrou $lock$. On attend donc un temps aléatoire avant de réessayer. Le gain en peformance augmente avec le nombre de coeurs. Portabilité et redimensionnement des délais difficile.

% section Verrou TATAS avec attente (end)

\section{Verrou ticket à attente proportionnelle} % (fold)
\label{sec:Verrou ticket à attente proportionnelle}

\begin{ccode}
    typedef struct
    {
        unsigned nextTicket;
        unsigned nowServing;
    } LOCK;

    LOCK lock = {0U,0U};

    void TicketAcquireLock(LOCK *lock)
    {
        unsigned myTicket = FetchAndIncrement(lock->nextTicket);
        while (true)
            if (lock->nowServing == myTicket)
                return;
            else
                sleep(myTicket - lock->nowServing);
    }

    void TicketReleaseLock(LOCK *lock)
    {
        lock->nowServing += 1;
    } 
\end{ccode}

L'idée revient à prendre un numéro et à attendre son tour, l'implémentation utilise l'instruction atomique \texttt{FetchAndIncrement} :

\begin{ccode}
    int FetchAndIncrement(int *num)
    {
        int pred = *num;    // fetch
        *num += 1;          // increment
        return pred;
    } 
\end{ccode}

% section Verrou ticket à attente proportionnelle (end)

\section{Verrou Anderson} % (fold)
\label{sec:Verrou Anderson}

L'idée est de réaliser une file FIFO où chaque coeur boucle sur une variable qui lui est propre. L'implémentation utilise deux instructions atomiques : \texttt{AtomicAdd} et \texttt{FetchAndIncrement} :

\begin{ccode}
    int AtomicAdd(int *num, int inc)
    {
        return *num += inc;
    }    
\end{ccode}

Algorithme :

\begin{algorithm}[h] %{{{
    \KwData{Un tableau de booléen : $slots$, avec la première case à true et les autres à false}
    \KwData{Un $int$ : $nextSlot$, la prochaine case de libre}
    $T_1$ acquiert le verrou par un \texttt{FetchAndIncrement} sur \verb|slot[nextSlot]|, ce qui retourne true et incrémente \verb|nextSlot|\;
    $T_2$ veut acquérir le verrou, il fait un $FAI$ mais obtient false. \verb|nextSlot| est incrémenté quand même\;
    $T_2$ attend car sont slot est à false\;
    $T_1$ rend le verrou est mettant son slot + $1$ à true\;
    $T_2$ voit true dans son slot et commence sa SC\;
    \caption{Verrou Anderson}
    \label{algo:verrou-anderson}
\end{algorithm} %}}}

\begin{ccode}
    typedef struct
    {
        bool *slots;
        int nextSlot;
    } LOCK;

    int numThreads;

    LOCK *AndersonCreateLock(void)
    {
        LOCK *lock;
        int i;
        if ((lock = (LOCK *)malloc(sizeof(LOCK))) != NULL)
            if (lock->slots = (bool *)malloc(numThreads *sizeof(bool)))
            {
                lock->slots[0] = true;
                for (i = 1; i < numThreads; i += 1)
                lock->slots[i] = false;
                lock->nextSlot = numThreads;
            }
            else
            {
                free(lock); lock = NULL;
            }
        return lock;
    }

    int AndersonAcquireLock(LOCK *lock)
    {
        int mySlot = FetchAndIncrement(&lock->nextSlot);

        if (mySlot % numThreads == 0)
            AtomicAdd(&lock->nextSlot,-numThreads);
        mySlot %= numThreads;

        while (!lock->slots[mySlot]);

        lock->slots[mySlot] = false;
        return mySlot;
    }

    void AndersonReleaseLock(LOCK *lock, int mySlot)
    {
        lock->slots[(mySlot+1) % numThreads] = true;
    }
\end{ccode}

Ce verrou nécessite un tableau par verrou et $1$ mot pou représenter le verrou : pour $V$ verrous et $T$ threads, il faut $V(T+1)$ mots. Le nombre de threads de l'application doit être statique.

% section Verrou Anderson (end)

\section{Verrou Graunke et Thakkar} % (fold)
\label{sec:Verrou Graunke et Thakkar}

Même propriétés que le verrou d'Anderson mais avec une instruction atomique en moins, lergèrement plus performant. Utilise l'instruction atomique \texttt{FetchAndStore}.

\begin{ccode}
    int FetchAndStore(int *adr, int val)
    {
        int ret = *adr;  // fetch
        *adr = val;      // store
        return ret;
    } 
\end{ccode}

\begin{algorithm}[h] %{{{
    \KwData{Un tableau de booléen : $slots$, initialisé à true}
    \KwData{Un pointeur sur bool : $tail$, ontient le dernier thread avec le verrou}
    \KwData{Un bool $whatIsLocked$ initilisé à false}
    $T_1$ essaie d'acquérir le verrou : FetchAndStore sur un tuple \texttt{(tail,whatIsLocked)} et récupère \texttt{(before,spin)}\;
    $T_1$ boucle sur \texttt{spin == *before} \tcp*{attente active}
    $T_1$ rentre en section critique\;
    $T_2$ essaie d'acquérir le verrou\;
    $T_2$ boucle sur \texttt{spin == *before} \tcp*{attente active}
    $T_1$ met \texttt{slots[id] = !slots[id]}
    $T_2$ aquiert le verrou et rentre en section critique\;
    \caption{Verrou Graunke et Thakkar}
    \label{algo:verrou-graunke-thakkar}
\end{algorithm} %}}}

C'est algorithme est possible car l'implémentation se fie sur le fait que les adresses sont alignées sur des adresses paires. La variable \texttt{tail} peut alors être fusionnée avec \texttt{whatIsLocked} : le dernier bit de l'adresse prend alors la valeur de \texttt{whatIsLocked}.

\begin{ccode}
    typedef struct
    {
        bool *slots;
        bool *tail;
    } LOCK;

    int numThreads;
    LOCK *GraunkeThakkarCreateLock(void)
    {
        LOCK *lock;
        int i;
        if ((lock = (LOCK *)malloc(sizeof(LOCK))) != NULL)
            if ((lock->slots = (bool *)malloc(numThreads *sizeof(bool))) != NULL)
            {
                for (i = 0; i < numThreads; i += 1)
                    lock->slots[i] = 0x1;
                lock->tail = &lock->slots[0];
            }
            else
            {
                free(lock); lock = NULL;
            }
        return lock;
    }

    void GraunkeThakkarAcquireLock(LOCK *lock, int threadID)
    {
        bool *before, *mySlot, spin;
        mySlot = &lock->slots[threadID] | lock->slots[threadID];
        before = (bool *)FetchAndStore(lock->tail,mySlot);
        
        spin = before & 0x1;
        before ^= spin;
        while (spin == *before);
    }

    void GraunkeThakkarReleaseLock(LOCK *lock, int threadID)
    {
        lock->slots[threadID] ^= 0x1;
    }
\end{ccode}

% section Verrou Graunke et Thakkar(end)

\section{Verrou MCS} % (fold)
\label{sec:Verrou MCS}

L'idée est de former une list chaînée représentant l'ordre des requêtes où chaque thread boucle sur une variable différente qu'ilmet à disposition. Cela utilse l'instruction \texttt{CompareAndSwap}:

\begin{ccode}
    bool CompareAndSwap(int *adr, int expectedVal, int newVal)
    {
        if (*adr == expectedVal)
        {
            *adr = newVal;
            return true;
        }
        else
            return false;
    }
\end{ccode}

\begin{algorithm}[h] %{{{$
    \KwData{Un id \texttt{id}}
    \KwData{Un noeud avec un booléen \texttt{locked} et un pointeur sur le suivant \texttt{next}}
    \KwData{Un noeud global \texttt{lock}}
    $T_1$ tente d'acquérir le verrou, il fait un \texttt{FetchAndStore} sur \texttt{lock} et stocke son propre noeud\;
    $T_1$ rentre en section critique car \texttt{lock} vaut \texttt{NULL}\;
    $T_2$ tente d'acquérir le verrou, il fait un \texttt{FetchAndStore} sur \texttt{lock} et stocke son propre noeud\;
    $T_2$ boucle sur son propre noeud car \texttt{lock} n'est pas \texttt{NULL}\;
    $T_1$ relache le verrou et effectuant un \texttt{CompareAndSwap} sur \texttt{(lock,myNode,NULL)}\;
    \caption{Verrou MCS}
    \label{algo:verrou-mcs}
\end{algorithm} %}}}

\begin{ccode}
    typedef struct QNODE
    {
        struct QNODE *next;
        bool locked;
    } QNODE;

    int numThreads;
    QNODE *lock = NULL;
    QNODE qnodes[numThreads];

    void MCSAcquireLock(QNODE **lock, int threadId)
    {
        QNODE *predecessor, *myNode = &qnodes[threadId];
        myNode->next = NULL;
        predecessor = FetchAndStore(lock,myNode);
        if (predecessor != NULL)
        {
            myNode->locked = true; predecessor->next = myNode;
            while (myNode->locked);
        }
    }

    void MCSReleaseLock(QNODE **lock, int threadId)
    {
        QNODE *myNode = &qnodes[threadId];
        if (myNode->next == NULL)
        {
            if (CompareAndSwap(lock,myNode,NULL))
                return;
            while (myNode->next == NULL);
        }
        myNode->next->locked = false;
    } 
\end{ccode}

% section Verrou MCS (end)

\section{Verrou CLH} % (fold)
\label{sec:Verrou CLH}

Utilise l'instruction atomique \texttt{FetchAndStore}, utilise $2$ mots par thread et $3$ mots par verrou. Pour $V$ verrous et $T$ threads, nous avons donc $3V+2T$ mots. Un peu plus rapide que MCS à cause des lectures en moins.

\begin{algorithm}[h] %{{{
    \KwData{Un noeud par thread avec un pointeur sur le prochain noeud \texttt{next} et un booléen \texttt{locked}}
    \KwData{Un noeud global \texttt{lock} qui pointe vers un autre noeud à \texttt{true}}
    $T_1$ tente d'acquérir le verrou, il fait un \texttt{CAS} sur le noeud pointé par \texttt{lock}\;
    $T_1$ entre en section critique si ce qu'il récupère vaut n'est pas lock\;
    $T_2$ tente d'acquérir le verrou, il fait un \texttt{CAS} sur le noeud pointé par \texttt{lock}\;
    $T_2$ boucle sur son lock car il n'est pas libre\;
    $T_1$ libère le verrou en mettant son \texttt{next} à \texttt{true}\;
    \caption{Verrou CLH}
    \label{algo:verrou-clh}
\end{algorithm} %}}}

% section Verrou CLH (end)

% chapter Exclusion mutuelle (end)

\chapter{Objets concurrents} % (fold)
\label{cha:Objets concurrents}

En programmation concurrente, lorsque l'on fait appel à un objet et qu'on utilise des méthodes sur celui-ci, les instructions dans la même méthodes peuvent se chevaucher mais aussi pour l'ensemble des méthodes de l'objet. Pour prouver un objet, il faut donc prendre en compte toutes les interactions possibles, aussi quand on rajoute des méthodes.

Sur un file FIFO avec verrou, chaque modification de l'objet se fait par exclusion mutuelle :
\begin{itemize}
    \item pas d'action qui se chevauche
    \item même comportement qu'en séquentiel
\end{itemize}

L'idée est que pour un comportement concurrent, un équivalent séquentiel. Chaque action réalisée sur un objet doit :
\begin{itemize}
    \item prendre effet
    \item paraître instantanée entre son invocation et son retour
\end{itemize}

un tel objet est dit \textit{atomique}. Définitions :
\begin{itemize}
    \item L'invocation d'une méthode est définie par $2$ événements : son invocation et sa réponse.
    \item La durée d'une méthode est un intervalle débutant par son invocation et terminant par sa réponse.
    \item Une invocation et une réponse sont dites couplées si la réponse correspond à l'invocation.
    \item L'invocation d'une méthode est dite pendante si sa réponse n'a pas encore eu lieu.
    \item Un historique est une suite d'invocation et de réponse.
    \item Un historique est complet si toutes ses invocations sont couplées à leur réponse, sinon il est partiel.
\end{itemize}

\section*{Notation :}

Inocation : \verb|<thread><objet>.<méthode>(<arguments>)|

Réponse : \verb|<thread><objet>:<résultat>|

Un historique est dit séqentiel quand son premier événement est une invocation que toutes les invocations et réponses sont couplées. Un historique qui n'est pas séquentiel est dit concurrent. Pas tous les historiques ont un sens, seuls les historiques filtrés par threads donnant des sous historiques séquentiels sont permis.

Deux historiques sont dit \textit{équivalent} si pour tous les threads $A$ de $H$, $H|A=G|A$.

Un historique $H$ est légal si pour tous les objets $x$ dans $H$,$H|x$ satisfait à la spécification séquentille de $x$. Un historique séquentiel est simple à vérifier car l'historique définit un ordre total.

Comme les threads ne réalisent pas que des historiques séquentiels, il faut transformer un historique concurrent en un historique séquentiel équivalent.

\section{Précédence} % (fold)
\label{sec:Précédence}

Une opération sur un objet précède une autre si la réponse de la première précède l'invocation de la seconde, une opération sur un objet chevauche une autre si la réponse de la première ne précède pas l'invocation de la seconde.

% section Précédence (end)

\section{Atomicité} % (fold)
\label{sec:Atomicité}

Un historique $H$ est atomique si $H$ peut être étendu à un historique $G$ tel que $G$ est équivalent à un historique séquentiel $S$. Remarques :
\begin{itemize}
    \item La transformation de $H$ à $G$ permet de tenir compte de tous les appels pendants.
    \item L'historique $S$ est une atomisation de $H$.
    \item L'historique $H$ peut avoir plusieurs atomisations $S$.
\end{itemize}

Méthodes :
\begin{itemize}
    \item Identifier les points d'atomicités (mettre les traits bleu au endroit logique);
    \item Traiter les opérations pendantes (si point d'atomicité atteint, on écrit le retour de la méthode à la fin de l'historique, sinon on la supprime);
    \item Trouver un historique séquentiel équivalent.
\end{itemize}

% section Atomicité (end)

% chapter Objets concurrents (end)

\end{document}
